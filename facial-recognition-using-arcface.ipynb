{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12312091,"sourceType":"datasetVersion","datasetId":7760463},{"sourceId":12320420,"sourceType":"datasetVersion","datasetId":7751158}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt install tree -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T08:32:28.935986Z","iopub.execute_input":"2025-06-30T08:32:28.936881Z","iopub.status.idle":"2025-06-30T08:32:38.470423Z","shell.execute_reply.started":"2025-06-30T08:32:28.936846Z","shell.execute_reply":"2025-06-30T08:32:38.469407Z"}},"outputs":[{"name":"stdout","text":"Reading package lists...\nBuilding dependency tree...\nReading state information...\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 87 not upgraded.\nNeed to get 47.9 kB of archives.\nAfter this operation, 116 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\nFetched 47.9 kB in 0s (138 kB/s)\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package tree.\n(Reading database ... 129184 files and directories currently installed.)\nPreparing to unpack .../tree_2.0.2-1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking tree (2.0.2-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up tree (2.0.2-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Processing triggers for man-db (2.10.2-1) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# list of all annotated faces cropped from dataset\n!tree --filelimit 10  /kaggle/input/2025-06-26-apscscl-instance-seg-yolo/2025-06-27-class_crops/class_crops/face","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T08:32:38.472393Z","iopub.execute_input":"2025-06-30T08:32:38.472660Z","iopub.status.idle":"2025-06-30T08:32:40.270312Z","shell.execute_reply.started":"2025-06-30T08:32:38.472632Z","shell.execute_reply":"2025-06-30T08:32:40.269142Z"}},"outputs":[{"name":"stdout","text":"\u001b[01;34m/kaggle/input/2025-06-26-apscscl-instance-seg-yolo/2025-06-27-class_crops/class_crops/face\u001b[0m  [3527 entries exceeds filelimit, not opening dir]\n\n0 directories, 0 files\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def display(path):\n    import cv2\n    from matplotlib import pyplot as plt\n    \n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n    \n    plt.figure(figsize=(2, 2))  # Increase figure size\n    plt.imshow(img)\n    plt.axis('off')  # Hide axes\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T08:32:40.274635Z","iopub.execute_input":"2025-06-30T08:32:40.274890Z","iopub.status.idle":"2025-06-30T08:32:40.281200Z","shell.execute_reply.started":"2025-06-30T08:32:40.274859Z","shell.execute_reply":"2025-06-30T08:32:40.280190Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# lists all contents\n# !ls /kaggle/input/2025-06-26-apscscl-instance-seg-yolo/2025-06-27-class_crops/class_crops/face","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T08:32:40.283341Z","iopub.execute_input":"2025-06-30T08:32:40.283612Z","iopub.status.idle":"2025-06-30T08:32:40.303687Z","shell.execute_reply.started":"2025-06-30T08:32:40.283591Z","shell.execute_reply":"2025-06-30T08:32:40.302710Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"display(\"/kaggle/input/2025-06-26-apscscl-instance-seg-yolo/2025-06-27-class_crops/class_crops/face/yemiganuru_ch7_20250424090205_20250424090436_frame_90s_01992.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T08:32:40.304678Z","iopub.execute_input":"2025-06-30T08:32:40.304928Z","iopub.status.idle":"2025-06-30T08:32:41.020591Z","shell.execute_reply.started":"2025-06-30T08:32:40.304902Z","shell.execute_reply":"2025-06-30T08:32:41.018940Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 200x200 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!pip install deepface -q\nprint(\"✅ DeepFace installation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T08:32:41.021611Z","iopub.execute_input":"2025-06-30T08:32:41.022175Z","iopub.status.idle":"2025-06-30T08:32:51.072315Z","shell.execute_reply.started":"2025-06-30T08:32:41.022115Z","shell.execute_reply":"2025-06-30T08:32:51.071178Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n✅ DeepFace installation complete.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport shutil\n# --- HIDE HARMLESS TENSORFLOW WARNINGS ---\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n\nimport matplotlib.pyplot as plt\nimport cv2\nfrom deepface import DeepFace\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', None)\n\n# --- DEFINE ALL PATHS AND PARAMETERS ---\n\n# 1. Path to the MAIN database of 3500+ faces to search through\nSRC_DB_PATH = '/kaggle/input/2025-06-26-apscscl-instance-seg-yolo/2025-06-27-class_crops/class_crops/face/'\n\n# 2. Path to the FOLDER containing your 12 area reference face folders\nREFERENCE_FACES_PATH = '/kaggle/input/referencedata/faces/'\n\n# 3. Writable location for the database (DeepFace needs to write a .pkl file here)\nWRITABLE_DB_PATH = '/kaggle/working/face_db/'\n\n# --- Parameters for the dashboard ---\nTOP_N_MATCHES = 10\nARCFACE_THRESHOLD = 0.68 # Lower is more similar. Anything above this is likely not a match.\n\n#------------------------------------------------------------------------------------\n# PART 1: ONE-TIME DATABASE SETUP (This will take ~16 mins ONCE)\n#------------------------------------------------------------------------------------\n\n# Check if the processed database file already exists\npickle_file_path = os.path.join(WRITABLE_DB_PATH, 'ds_model_arcface_detector_opencv_aligned_normalization_base_expand_0.pkl')\n\nif os.path.exists(pickle_file_path):\n    print(f\"✅ Pre-processed database found at '{WRITABLE_DB_PATH}'. Skipping setup.\")\nelse:\n    print(\"--- Starting One-Time Database Setup ---\")\n    \n    # Copy the main database to a writable location if it's not already there\n    if not os.path.exists(WRITABLE_DB_PATH):\n        print(f\"Copying main database from '{SRC_DB_PATH}' to '{WRITABLE_DB_PATH}'...\")\n        shutil.copytree(SRC_DB_PATH, WRITABLE_DB_PATH)\n        print(\"Copy complete.\")\n    else:\n        print(f\"Writable database directory already exists at '{WRITABLE_DB_PATH}'.\")\n        \n    # This is the crucial step that processes all 3500+ images and creates the .pkl file.\n    print(\"Processing all faces in the main database to create embeddings.\")\n    print(\"This will take approximately 16 minutes. Please be patient...\")\n    # We run find() once to force the creation of the embeddings file.\n    try:\n        DeepFace.find(\n            img_path=np.zeros([10, 10, 3]), # A dummy black image\n            db_path=WRITABLE_DB_PATH,\n            model_name='ArcFace',\n            enforce_detection=False\n        )\n        print(\"✅✅ One-Time Database Setup Complete! All future searches will be fast.\")\n    except Exception as e:\n        print(f\"An error occurred during database setup: {e}\")\n        print(\"Cannot proceed without a processed database.\")\n        # Stop execution if setup fails\n        exit()\n\n#------------------------------------------------------------------------------------\n# PART 2: GENERATE THE SIMILARITY DASHBOARDS\n#------------------------------------------------------------------------------------\n\nprint(\"\\n\\n--- Starting Dashboard Generation ---\")\n\narea_folders = sorted([d for d in os.listdir(REFERENCE_FACES_PATH) if os.path.isdir(os.path.join(REFERENCE_FACES_PATH, d))])\n\n# Loop through each area folder (Bapatla, Guduru, etc.)\nfor area in area_folders:\n    print(\"=\"*80)\n    print(f\"      GENERATING DASHBOARD FOR: {area.upper()}\")\n    print(\"=\"*80)\n    \n    current_area_path = os.path.join(REFERENCE_FACES_PATH, area)\n    ref_images = sorted(os.listdir(current_area_path))\n    \n    # Identify roles based on filenames\n    people_to_process = []\n    for img_name in ref_images:\n        role = \"Hamali\"\n        if '-incharge' in img_name:\n            role = \"Incharge\"\n        elif '-deo' in img_name:\n            role = \"DEO\"\n        \n        people_to_process.append({\n            'role': role,\n            'filename': img_name,\n            'path': os.path.join(current_area_path, img_name)\n        })\n\n    num_people = len(people_to_process)\n    if num_people == 0:\n        continue\n\n    # Create a large plot for the entire area's dashboard\n    fig, axs = plt.subplots(num_people, 1 + TOP_N_MATCHES, figsize=(22, num_people * 2.2))\n    # If there's only one person, axs is not a 2D array, so we fix it\n    if num_people == 1:\n        axs = np.array([axs])\n\n    # Loop through each person in the current area\n    for i, person in enumerate(people_to_process):\n        \n        # --- Display the reference image in the first column ---\n        ref_img = cv2.imread(person['path'])\n        ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)\n        axs[i, 0].imshow(ref_img)\n        axs[i, 0].set_title(f\"{person['role']}\\n{person['filename']}\", fontsize=10)\n        axs[i, 0].axis('off')\n\n        try:\n            # Find the top matches for this person\n            df_list = DeepFace.find(\n                img_path=person['path'],\n                db_path=WRITABLE_DB_PATH,\n                model_name='ArcFace',\n                enforce_detection=False\n            )\n\n            # Filter the results by the distance threshold\n            if df_list and not df_list[0].empty:\n                df = df_list[0]\n                good_matches_df = df[df['distance'] <= ARCFACE_THRESHOLD].head(TOP_N_MATCHES)\n            else:\n                good_matches_df = pd.DataFrame() # Empty dataframe\n\n            # --- Display the top N matches in the other columns ---\n            for j in range(TOP_N_MATCHES):\n                ax = axs[i, j + 1]\n                if j < len(good_matches_df):\n                    match_path = good_matches_df.iloc[j]['identity']\n                    distance = good_matches_df.iloc[j]['distance']\n                    \n                    match_img = cv2.imread(match_path)\n                    match_img = cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB)\n                    ax.imshow(match_img)\n                    \n                    # Convert distance to similarity percentage\n                    similarity_percent = (1 - distance) * 100\n                    \n                    # Special title for the first 3 matches\n                    if j < 3:\n                        ax.set_title(f\"Sim: {similarity_percent:.1f}%\\n(Dist: {distance:.2f})\", fontsize=9, color='green')\n                    else:\n                         ax.set_title(f\"Dist: {distance:.2f}\", fontsize=9)\n                else:\n                    # If there are no more matches, display a black box\n                    ax.imshow(np.zeros((100, 100, 3), dtype=np.uint8))\n                    ax.text(0.5, 0.5, 'No Match', color='white', ha='center', va='center')\n                \n                ax.axis('off')\n\n        except Exception as e:\n            print(f\"Error processing {person['filename']}: {e}\")\n\n    plt.tight_layout(pad=0.5, h_pad=1.0)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}